<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Regression and its many forms</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Science Notes</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Sections
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="newlab2.html">Intro to R</a>
    </li>
    <li>
      <a href="newlab3.html">Commands and functions</a>
    </li>
    <li>
      <a href="newlab4.html">Data massaging: basics</a>
    </li>
    <li>
      <a href="newlab5.html">Intro to programming</a>
    </li>
    <li>
      <a href="newlab6.html">ggplot2</a>
    </li>
    <li>
      <a href="newlab7.html">Data transformations I</a>
    </li>
    <li>
      <a href="newlab8.html">Data transformations II</a>
    </li>
    <li>
      <a href="newlab9.html">Data cleaning</a>
    </li>
    <li>
      <a href="newlab10.html">Random variables</a>
    </li>
    <li>
      <a href="newlab11.html">Discrete and continous random variables</a>
    </li>
    <li>
      <a href="newlab12.html">Mean and variance</a>
    </li>
    <li>
      <a href="newlab14.html">Confidence Intervals</a>
    </li>
    <li>
      <a href="newlab15.html">Hypothesis Testing</a>
    </li>
    <li>
      <a href="newlab16.html">Regression</a>
    </li>
    <li>
      <a href="newlab18.html">Machine learning</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Regression and its many forms</h1>

</div>


<p>This lab will be focused on the many ways to perform
<strong>regression</strong> on single variable data. From the previous
notes, we learned about <strong>linear regression</strong>. For a
dataset <span class="math inline">\(\{(x_1, y_1), \dots, (x_n,
y_n)\}\)</span>, linear regression uses the linear model of <span
class="math display">\[\begin{equation}
y = \alpha+\beta x, \quad \alpha = \bar y-\frac{\rho s_y}{s_x}\bar x,
\quad
\beta = \frac{\rho s_y}{s_x}.
\end{equation}\]</span></p>
<p>where we use the <strong>correlation coefficient</strong></p>
<p><span class="math display">\[\begin{equation}
\rho_{X,Y} = \frac{\sum (x_i-\bar x)(y_i- \bar y)}{\sqrt{\sum (x_i-\bar
x)^2}
\sqrt{\sum (x_i-\bar x)^2}}
\end{equation}\]</span></p>
<p>Recall that <span class="math inline">\(\rho\)</span> gives us a
measure of the <em>linear</em> relationship between input and output.
For nonlinear relations (parabolas, circles, etc.), the correlation
coefficient will not be as effective (you may even get something near
0!). The terms <span class="math inline">\(\bar x, \bar y\)</span> are
means of the input and output, respectively, and <span
class="math inline">\(s_x, s_y\)</span> their sample standard
deviations.</p>
<p>Let’s take a quick look at some more subtle points regarding
correlation and linear regression.</p>
<div id="corrupting-rho-with-a-single-point" class="section level2"
number="0.1">
<h2><span class="header-section-number">0.1</span> Corrupting <span
class="math inline">\(\rho\)</span> with a single point</h2>
<p>Unfortunately, correlations and linear fits are sensitive to
outliers. To see this, let’s make some <em>noise</em>!<a href="#fn1"
class="footnote-ref" id="fnref1"><sup>1</sup></a> By this, generate
independent samples of input and output.</p>
<pre class="r"><code>X = runif(100)
Y = runif(100)

cor(X,Y)</code></pre>
<pre><code>## [1] 0.1051301</code></pre>
<blockquote>
<p><strong>Q:</strong> Shouldn’t we be expecting exactly 0 as an answer?
Why or why not?</p>
</blockquote>
<p>By adding a single point, we can boost our correlation to be as near
to 1 as we wish.</p>
<pre class="r"><code>X[101] = 10^5
Y[101] = 10^5

cor(X,Y)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>What on earth is going on? How can one point mess things up so much?
Looking at the formula for <span class="math inline">\(\rho\)</span> can
help. For the augmented dataset</p>
<p><span class="math display">\[\begin{equation}
\mathbf x = \{x_1, \dots, x_n, x_{big}\}, \quad \mathbf x = \{y_1,
\dots, y_n, y_{big}\},
\end{equation}\]</span> with <span class="math inline">\(x_{big} \gg
x_i\)</span>, <span class="math inline">\(y_{big} \gg y_i\)</span> The
correlation coefficient is then approximated by just looking at those
terms containing <span class="math inline">\(x_{big}\)</span> and <span
class="math inline">\(y_{big}\)</span>.</p>
<blockquote>
<p><strong>Q:</strong> Compute the <em>big terms</em> in the numerator
and denominator in the formula for the correlation coefficient. In the
numerator, you should be looking for terms containing <span
class="math inline">\(x_{big}*y_{big}\)</span>, and in the denominator,
you should be looking for terms <span
class="math inline">\(x_{big}^2\)</span> and <span
class="math inline">\(y_{big}^2\)</span>. Can you find these terms and
show that their ratio approaches 1 for large positive values of <span
class="math inline">\(x_{big}\)</span> and <span
class="math inline">\(y_{big}\)</span>?</p>
</blockquote>
<p>Without doing the <em>asymptotics</em> of the previous question, we
can get a visual of what our line should look like. Let’s plot our
points and the line of best fit.</p>
<pre class="r"><code>rho = cor(X,Y)
sx = sd(X)
sy = sd(Y)
xbar = mean(X)
ybar = mean(Y)

riggeddata = data.frame(X,Y)
linefit = function(x){
alpha = ybar- rho*(sy/sx)*xbar
beta = rho*(sy/sx)
return (alpha + beta*x)}

riggeddata %&gt;% ggplot(aes(X,Y)) + geom_point()+stat_function(fun = linefit, col = &#39;red&#39;)</code></pre>
<p><img src="newlab16_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The other 100 points are so scrunched that they appear to be a single
point in this graph. This graph, in a sense, is <em>zooming</em> out,
and from this perspective, the line of best fit seems to do a pretty
good job. Recall that correlations are invariant under <em>affine
transmations</em> <span class="math inline">\(x \mapsto ax+b\)</span>,
so the scaling in the <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> variables doesn’t really matter when
looking at something like a correlation.</p>
</div>
<div id="x-vs.-y-vs.-y-vs.-x" class="section level2" number="0.2">
<h2><span class="header-section-number">0.2</span> (<span
class="math inline">\(X\)</span> vs. <span
class="math inline">\(Y\)</span>) vs. (<span
class="math inline">\(Y\)</span> vs. <span
class="math inline">\(X\)</span>)</h2>
<p>One nice property about the correlation coefficient is that it is
<strong>symmetric</strong>, meaning that <span
class="math inline">\(\rho(\mathbf x,\mathbf y) = \rho(\mathbf y,\mathbf
x)\)</span>.</p>
<blockquote>
<p><strong>Q:</strong> Can you deduce that <span
class="math inline">\(\rho\)</span> is symmetric by looking at its
formula?</p>
</blockquote>
<p>Despite <span class="math inline">\(\rho\)</span> being symmetric, we
have the curious fact that the line of best fit for <span
class="math inline">\(Y\)</span> given <span
class="math inline">\(X\)</span> is in general not the same as the line
for <span class="math inline">\(X\)</span> given <span
class="math inline">\(Y\)</span>. To see this, let’s use our trusty
&lt;<T>&gt;iris&lt;</T>&gt; dataset, and compare sepal lengths against
sepal widths.</p>
<pre class="r"><code>X = iris$Sepal.Length
Y = iris$Petal.Length

rho = cor(X,Y)
sx = sd(X)
sy = sd(Y)
xbar = mean(X)
ybar = mean(Y)

linefit = function(x){
alpha = ybar- rho*(sy/sx)*xbar
beta = rho*(sy/sx)
return((alpha + beta*x))
}



#Now computing the line of best fit for X given Y:

rho = cor(Y,X)


fliplinefit= function(x){
alpha = xbar- rho*(sx/sy)*ybar
beta = rho*(sx/sy)
return((x-alpha)/beta)
}

iris %&gt;% ggplot(aes(Sepal.Length, Petal.Length)) + 
geom_point()+stat_function(fun = linefit, col = &#39;red&#39;)+
  stat_function(fun = fliplinefit, col = &#39;green&#39;)</code></pre>
<p><img src="newlab16_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The main point here is that switching arguments produces two separate
optimization problems. In the graph above, the red line is minimizing
<strong>vertical residuals</strong>, whereas the green line minimizes
<strong>horizontal residuals</strong>.</p>
<p>#Polynomial regression</p>
<p>We can easily generalize linear regression to cover the class of
<strong>polynomials</strong>, those functions which take the form <span
class="math display">\[\begin{equation}
\beta_Nx^d+\beta_{N-1}x^{N-1}+ \dots +\beta_1 x+ \beta_0 = \sum_{i =
0}^N \beta_i x^i
\end{equation}\]</span></p>
<p>the variable <span class="math inline">\(N\)</span> is called the
<strong>degree</strong> of the polynomial, and is the highest power that
we take of the independent variable <span
class="math inline">\(x\)</span>. For fitting a degree <span
class="math inline">\(N\)</span> polynomial to a dataset, our task is to
determine the best possible values for the <span
class="math inline">\(N+1\)</span> coefficients <span
class="math inline">\(\beta_0, \dots, \beta_N\)</span>. We can express
this as the optimization problem</p>
<p><span class="math display">\[\begin{equation}
(\hat \beta_0 , \dots, \hat \beta_N) = \mathrm{argmin}_{(\beta_0, \dots,
\beta_d)} \sum_{i = 1}^d (y(\hat x_i)-\hat y_i)^2 =
\mathrm{argmin}_{(\beta_0, \dots, \beta_N)} \sum_{i = 1}^d \left(\sum_{i
= 0}^ N \beta_i  \hat x^i-\hat y_i\right)^2.
\end{equation}\]</span></p>
<p>The argmin tells you to find values of <span
class="math inline">\((\hat \beta_0 , \dots, \hat \beta_N)\)</span> that
produce a minimum <strong>root mean squared error</strong> <span
class="math display">\[\begin{equation}
\mathrm{RMSE} = \sqrt{ \sum_{i = 1}^d (y(\hat x_i)-\hat
y_i)^2/d}
\end{equation}\]</span></p>
<p>These are the constants you will use to plot your polynomial of best
fit.</p>
</div>
<div id="an-example-with-tracking-coronavirus-cases"
class="section level1" number="1">
<h1><span class="header-section-number">1</span> An example with
tracking coronavirus cases</h1>
<p>Let’s take a look different regression models for tracking the total
number of coronavirus cases in India. We will use the
&lt;<T>&gt;coronavirus&lt;</T>&gt; library, which contains daily case
and death tallies over most countries.</p>
<p>First, let’s take a look at case counts from March 2020 to today.
Note that we’ll be using the &lt;<T>&gt;filter&lt;</T>&gt;function to
zero in on the data we find relevant for our problem.</p>
<pre class="r"><code>newcor = coronavirus %&gt;% filter(country %in% c(&quot;US&quot;)
                                &amp; type == &#39;confirmed&#39;&amp; date %in%  as.Date(1:450, origin=&quot;2020-04-01&quot;))

newcor %&gt;%
  ggplot(aes(x = date, y = cases))+ 
  geom_bar( stat = &#39;identity&#39;)+
  geom_smooth(color = &#39;pink&#39;, span = .3)  </code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="newlab16_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The &lt;<T>&gt;geom_/smooth&lt;</T>&gt; function is using what’s
called <em>Loess</em>, or <em>local regression</em>. We’ll get to what
that is shortly, but for now let’s just take a look at the data. Like
many things in the real world, it’s not clear what type of function
would best fit a function like we see above. For sure, it’s not one of
the usual suspects we encounter in calculus (exponential, sine, cosine,
log, or any thing like that). Fortunately, though, it is the case that
any continuous function can be approximated arbitrarily well by a
polynomial, a result known as the <strong>Stone Weierstass
Theorem</strong>. The issue is that the degree of the polynomial might
be enormous. This is bad news for modeling, since we’d like to describe
what’s going on with as few variables as possible.</p>
<p>Just to make things look simpler, let’s work with a new variable
which gives the number of days since April 1.</p>
<pre class="r"><code>newcor$day = 1:dim(newcor)[1]

newcor %&gt;%
  ggplot(aes(x = day, y = cases))+ 
  geom_bar( stat = &#39;identity&#39;)+
  geom_smooth(color = &#39;pink&#39;, span = .3)  </code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="newlab16_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Now let’s do the unthinkable and actually try using linear
regression! We should expect a pretty horrific result, but it’ll be good
exercise on writing an explicit solution for an optimization problem,
something that we won’t have on hand when looking at more complicated
problems.</p>
<pre class="r"><code>X = newcor$day
Y = newcor$cases
rho = cor(X,Y)
sx = sd(X)
sy = sd(Y)
xbar = mean(X)
ybar = mean(Y)

linefit = function(x){
alpha = ybar- rho*(sy/sx)*xbar
beta = rho*(sy/sx)
return((alpha + beta*x))
}


newcor %&gt;% 
  ggplot(aes(x = day, y = cases))+ 
  geom_bar( stat = &#39;identity&#39;)+
  geom_smooth(color = &#39;green&#39;, span = .3) +
  stat_function(fun = linefit)</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="newlab16_files/figure-html/unnamed-chunk-9-1.png" width="672" />
Hideous as expected.</p>
<p>Let’s derive the result using a direct minimization. The idea here is
that we can generalize this method to regression for polynomials of
whatever degree we wish.</p>
<pre class="r"><code>model = function(a, data){
  
  a[1]+data$day*a[2]
}

#Here&#39;s the root mean squared distance function
rmsd = function(mod, data){
  
  diff = data$cases-model(mod, data)
  return(sqrt(mean(diff^2)))
}</code></pre>
<p>Now let’s find the minimum value of our parameters with
<strong>Newton’s method</strong>, the insanely fast algorithm for
finding <strong>zeros</strong> of a function (those numbers <span
class="math inline">\(c\)</span> such that <span
class="math inline">\(f(c) = 0\)</span>.)</p>
<pre class="r"><code>#We&#39;ll just plug in random conditions in for the initial conditions.
minparams = optim(c(0,0), rmsd, data = newcor)

minparams$par</code></pre>
<pre><code>## [1] 54052.49528    89.91705</code></pre>
<p>We’re basically done. What remains is simply plugging in our optimal
values into our linear model and comparing against the original
function</p>
<pre class="r"><code>newcor %&gt;%
  ggplot(aes(x = day, y = cases))+ 
  geom_bar( stat = &#39;identity&#39;)+
  geom_smooth(color = &#39;white&#39;, span = .3) +
  geom_abline(intercept = minparams$par[1], slope = minparams$par[2]) +
  stat_function(fun = linefit, col = &#39;red&#39;)</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="newlab16_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Same awful line!</p>
<blockquote>
<p><strong>Q:</strong> Hold on a second! Do we get different answers if
we plug in different initial conditions? The answer, sadly, is yes. Try
plugging in (0,0) as your initial conditions and see what happens. Any
ideas on what might be happening? If you get different answers for
different initial conditions, how do you determine which answer is the
best one?</p>
</blockquote>
<p>Certainly adding some degrees should help. Let’s try a cubic
model.</p>
<pre class="r"><code>#This model2 has 4 parameters:
model2 = function(a, data){
  
  a[1]+data$day*a[2]+data$day^2*a[3]+data$day^3*a[4]
}


rmsd2 = function(mod, data){
  
  diff = data$cases-model2(mod, data)
  return(sqrt(mean(diff^2)))
}

#Find the minimum value with Newton&#39;s method.  
#Guess initial values for algorithm are (0,0)
minparams = optim(c(0,0, 0, 0), rmsd2, data = newcor)

mincubic = function(x){
  
  return (minparams$par[1]+minparams$par[2]*x+minparams$par[3]*x^2+
            minparams$par[4]*x^3)    
}

newcor %&gt;%
  ggplot(aes(x = day, y = cases))+ 
  geom_bar( stat = &#39;identity&#39;)+
  geom_smooth(color = &#39;white&#39;, span = .3) +
  stat_function(fun = mincubic)</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="newlab16_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Not perfect, but certainly better! Let’s keep moving, and look at a
degree 8 model. That should help us capture the hills and valleys
better.</p>
<pre class="r"><code>newercor = newcor
newercor$day = newcor$day/dim(newcor)[1]
newercor$cases = newcor$cases/max(newcor$cases)





power = 16
#A for loop is convenient here for writing down a degree 8 polynomial
model3 = function(a, data){
  answer = 0
  for (i in 1:power){
    
    answer = answer+ a[i]*data$day^(i-1)
  }
  return(answer)
}


rmsd3 = function(mod, data){
  
  diff = data$cases-model3(mod, data)
  return(sqrt(mean(diff^2)))
}

#Find the minimum value with Newton&#39;s method.  
#Guess initial values for algorithm are (0,0)
minparams = optim(rep(0, power), rmsd3, data = newercor)

minoctic = function(x){
  answer = 0
  for (i in 1:power){
  answer = answer+ minparams$par[i]*x^(i-1)
}
return(answer)
}

newercor %&gt;%
  ggplot(aes(x = day, y = cases))+ 
  geom_bar( stat = &#39;identity&#39;)+
  geom_smooth(color = &#39;white&#39;, span = .3) +
  stat_function(fun = minoctic, color = &#39;red&#39;)</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="newlab16_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Still not quite there.</p>
<blockquote>
<p><strong>Q:</strong> The code we wrote will apply for any polynomial
we choose. Poke around with different degrees and see if you can get a
better fit.</p>
</blockquote>
<blockquote>
<p><strong>Q:</strong> Poke around some more at different countries.
What kinds of graphs do you think would be the easiest to fit? The
hardest?</p>
</blockquote>
</div>
<div id="local-regression" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Local regression</h1>
<p>We can also consider methods for building curves that approximate
functions locally.This means for a particular point <span
class="math inline">\(x\)</span>, my approximation <span
class="math inline">\(\hat f(x)\)</span> is based on points within a
neighborhood <span class="math inline">\(B_r(x) = [x-r, x+r]\)</span> of
radius <span class="math inline">\(r\)</span> centered at <span
class="math inline">\(x\)</span>.</p>
<p>More precisely, given a dataset <span class="math inline">\(D =
\{(x_1, y_1), \dots, (x_d, y_d)\}\)</span>, my estimated function takes
the form</p>
<p><span class="math display">\[\begin{equation}
\hat f(x)  = \frac{1}{|A_x(r)|}\sum_{i \in A_x(r)} y_i
\end{equation}\]</span></p>
<p>where <span class="math inline">\(A_x\)</span> is the set of indices
such that <span class="math inline">\(i \in A_x\)</span> means that
<span class="math inline">\(x_i\)</span> is an x-coordinate in the
dataset <span class="math inline">\(D\)</span>.</p>
<p>You might notice that smoothing tends to give nicer looking curves
than something like polynomial regression. So why every bother with
polynomial regression? In short, polynomial regression is a
parametrization. At its best, regression builds a model which can be
used to predict new data using only a small set of parameters, or
numbers that describe my model.</p>
<p>Here is a thought experiment for illustrating the utility of using
parameters. Suppose you want to calculate the height <span
class="math inline">\(h(t)\)</span> of a ball after dropping it from a
tower.</p>
<p>There are three ways to obtain this curve:</p>
<ol style="list-style-type: decimal">
<li><p>Begin with a theory of constant acceleration and derive position,
and test against experiment.</p></li>
<li><p>Take observations of the falling object and perform a quadratic
regression</p></li>
<li><p>Perform local averaging with the same data as in (2)</p></li>
</ol>
<p>Method (1) is good because it gives us an explanation for our curve
based on a simple assumption. Such an assumption can be used to try
explaining other phenomena. Method (2) provides us with approximate
constants for <span class="math inline">\(h(t)\)</span>, but regression
doesn’t give us a satisfying explanation for what’s going on. Local
averaging is very nice because it can be applied to basically any curve,
but its predictive power is clunky at best. One would have to store the
entire dataset to ensure a prediction if we have a new datapoint.</p>
<pre class="r"><code>newcor = coronavirus %&gt;% filter(country %in% c(&quot;US&quot;) 
                                &amp; type == &#39;death&#39;&amp; date %in%  as.Date(1:400, origin=&quot;2020-04-01&quot;))
newcor$day = 1:dim(newcor)[1]



span &lt;- 7

#Box Kernel
fit &lt;- ksmooth(newcor$date, newcor$cases, 
               kernel = &quot;box&quot;, bandwidth = span, n.points = dim(newcor)[1])


# #Normal Kernel
# fit &lt;- ksmooth(newcor$date, newcor$cases, 
#                kernel = &quot;normal&quot;, bandwidth = span, n.points = dim(newcor)[1])


newcor %&gt;% mutate(smooth = fit$y) %&gt;%
  ggplot(aes(date, cases)) +
  geom_bar( stat = &#39;identity&#39;) + 
  geom_line(aes(date, smooth), color=&quot;red&quot;)</code></pre>
<p><img src="newlab16_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<blockquote>
<p><strong>Q:</strong> This certainly beats the polynomial regression
models! Why would we ever bother with polynomial regression?</p>
</blockquote>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>No, I don’t mean this from the perspective of a
cheerleader.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
