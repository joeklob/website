<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Machine learning</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Science Notes</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Sections
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="newlab2.html">Intro to R</a>
    </li>
    <li>
      <a href="newlab3.html">Commands and functions</a>
    </li>
    <li>
      <a href="newlab4.html">Data massaging: basics</a>
    </li>
    <li>
      <a href="newlab5.html">Intro to programming</a>
    </li>
    <li>
      <a href="newlab6.html">ggplot2</a>
    </li>
    <li>
      <a href="newlab7.html">Data transformations I</a>
    </li>
    <li>
      <a href="newlab8.html">Data transformations II</a>
    </li>
    <li>
      <a href="newlab9.html">Data cleaning</a>
    </li>
    <li>
      <a href="newlab10.html">Random variables</a>
    </li>
    <li>
      <a href="newlab11.html">Discrete and continous random variables</a>
    </li>
    <li>
      <a href="newlab12.html">Mean and variance</a>
    </li>
    <li>
      <a href="newlab14.html">Confidence Intervals</a>
    </li>
    <li>
      <a href="newlab15.html">Hypothesis Testing</a>
    </li>
    <li>
      <a href="newlab16.html">Regression</a>
    </li>
    <li>
      <a href="newlab18.html">Machine learning</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Machine learning</h1>

</div>


<div id="machine-learning" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Machine learning</h1>
<p>The field of <strong>machine learning</strong> began to take hold
around the middle of the 20th century. Like the field of computer
science, mathematicians were at the forefront of laying the foundations
which have eventually led to the many creature comforts we are
experiencing today. Every time you cash a check at an ATM or on a cell
phone, for instance, a machine learning is at work, using loads of
previous written digits as training data to guess what you
<em>meant</em> to write with astounding accuracy. Almost every major
business uses machine learning to direct you to finding the right person
to speak to on the telephone (alright, alright, often that process is
incredibly annoying, but apparently it cuts down on total time on the
phone, or the business wouldn’t be using it). Phone apps use machine
learning to identify songs after a few seconds. There’s even an app for
identifying species by simply taking a snapshot in a garden! Machine
learning will be at the forefront of what types of jobs we will all be
doing in the future.</p>
<p>It won’t all be roses, of course. Plenty will be displaced, and it’s
a serious question of what we’re going to do with truckers when self
driving cars are prevalent. Other professions that you might not expect
may be on the chopping block too. Machine learning algorithms are
already beating doctors in identifying tumors, and there are plenty of
opportunities for automation to replace soldiers (this too, is loaded
with ethical dilemmas). It’s unfortunate that the reduction of labor,
what of first blush seems to be an absolute good, might cause a good
deal of hardship in the future. Data scientists are obviously going to
be needed in developing all of this automation, but we’ll also need very
bright people to figure out how to transition through this with as
little turbulence as possible.</p>
<p>Alright, enough of the doom and gloom. Let’s get into the meat of
machine learning. We’re going to go over the bare bones of two basic
methods in machine learning, <strong>k-means</strong> and
<strong>k-nearest neighbors</strong>.</p>
</div>
<div id="k-nearest-neighbors" class="section level1" number="2">
<h1><span class="header-section-number">2</span> <span
class="math inline">\(k\)</span>-nearest neighbors</h1>
<p>The algorithm of <span class="math inline">\(k\)</span>-nearest
neighbors (Knn) is a type of <strong>clustering algorithm</strong>. The
main question is as follows: Given a element with certain features,
predict a labeling. Knn, unlike logistic regression, is
<strong>nonparametric</strong>. The <span
class="math inline">\(k\)</span> in <span
class="math inline">\(k\)</span>-nearest neighbors gives a
<strong>tuning parameter</strong>. This is something we start off
with–in parametrized models we <em>learn</em> the parameter through
training.</p>
<p>The idea is that given a point, we predict its label by guessing the
labels of nearby datapoints in the training set. We can visualize a
decision boundary in two dimension, but it is still possible to preform
this in high dimensions.</p>
<p>A few points before we dig in:</p>
<ol style="list-style-type: decimal">
<li>The distance between two datapoints has a great deal of wiggle room.
We can define a distance using any <strong>metric</strong> <span
class="math inline">\(d(x,y)\ge 0\)</span> satisfying</li>
</ol>
<p>Examples abound. For two vectors, the most common metric is the
<strong>Euclidean metric</strong> <span
class="math display">\[\begin{equation}
d(\mathbf x, \mathbf y) =
\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+\dots+(x_n-y_n)^2}.
\end{equation}\]</span></p>
<center>
<div class="figure">
<img src="knn.png" alt="" />
<p class="caption">An example of knn with k = 3 and k = 5 (Image
courtesy of Wikipedia)</p>
</div>
</center>
<blockquote>
<p><strong>Q:</strong> Under different <span
class="math inline">\(k\)</span>, how ought we label the unclassified
green circle?</p>
</blockquote>
</div>
<div id="knn-on-the-iris-dataset" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Knn on the iris
dataset</h1>
<p>We’ll need the <TT>class</TT> package for running knn.</p>
<pre class="r"><code>library(class)</code></pre>
<pre class="r"><code># Some minor changes in the data set

# Normalizing variables

#Question: what does the -5 mean?
dataNorm = iris
dataNorm[,-5] = scale(iris[,-5])


#To make sure we can get repeatable (reproducible) results

set.seed(1234)


#This is picking (approximately) 70% of data to be training data.  
#What should you do if you always want fraction to be the same?
ind = sample(2, nrow(dataNorm), replace = TRUE, prob = c(.7, .3))

trainData = dataNorm[ind == 1, ]

testData = dataNorm[ind == 2, ]



#knn with k = 3
Knntestpredict = knn(trainData[,-5], testData[,-5],
                        trainData$Species, k = 3, prob = TRUE)

Knntestpredict</code></pre>
<pre><code>##  [1] setosa     setosa     setosa     setosa     setosa     setosa    
##  [7] setosa     setosa     setosa     setosa     versicolor versicolor
## [13] versicolor versicolor versicolor versicolor versicolor versicolor
## [19] versicolor versicolor versicolor versicolor virginica  virginica 
## [25] virginica  virginica  versicolor virginica  virginica  virginica 
## [31] virginica  virginica  versicolor virginica  virginica  virginica 
## [37] virginica  virginica 
## attr(,&quot;prob&quot;)
##  [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
##  [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
## [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
## [22] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
## [29] 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000
## [36] 1.0000000 0.6666667 1.0000000
## Levels: setosa versicolor virginica</code></pre>
<p>Here is the <strong>confusion matrix</strong></p>
<pre class="r"><code>table(testData$Species, Knntestpredict)</code></pre>
<pre><code>##             Knntestpredict
##              setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0         12         0
##   virginica       0          2        14</code></pre>
<p>The rows represent the actual species of plant, and the columns
represent the estimates from knn. The more entries on the diagonal, the
better. In our case, for <span class="math inline">\(k = 3\)</span>, we
have correctly classified <span class="math inline">\(10+12+14 =
36\)</span> plants. Two virginicas, however, have been misclassified as
versicolor.</p>
<blockquote>
<p><strong>Q:</strong> For different values of <span
class="math inline">\(k\)</span>, how does our confusion matrix perform.
Is it true that the higher the <span class="math inline">\(k\)</span>,
the better? What happens when <span class="math inline">\(k\)</span>
becomes very large?</p>
</blockquote>
</div>
<div id="k-means" class="section level1" number="4">
<h1><span class="header-section-number">4</span> K-means</h1>
<p>The history of k-means is quite old: it goes at least as far back as
the Polish mathematician Hugo Steinhaus in the 1950’s. The point of
<span class="math inline">\(k\)</span>-means is to do clustering without
training. This means that our data is not labeled. It is simply a
collection of points in some space. After initializing with <span
class="math inline">\(k\)</span> centroids, the two steps to k-means
are</p>
<ol style="list-style-type: decimal">
<li><p>(Labeling) Compute a <strong>Voronoi diagram</strong> for
centroids <span class="math inline">\(x_1, \dots, x_k\)</span>. This
produces cells which partition <span class="math inline">\(C_1, \dots,
C_k\)</span> the plane and have the minimum distance property <span
class="math display">\[\begin{equation}
x\in C_k \Rightarrow d(x,x_k) \le d(x,x_i), \quad i \neq k.
\end{equation}\]</span></p></li>
<li><p>(Update) Reposition centroids to the average position of points
in the Voronoi cells. <span class="math display">\[\begin{equation}
\frac{1}{|C_k|}\sum_{x \in C_k} x\leftarrow x_k .
\end{equation}\]</span></p></li>
</ol>
<p>The goal is to try clustering points which are close to each other.
For a measure, we want to minimize</p>
<p><span class="math display">\[\begin{equation}
\sum_{i = 1}^k \sum_{x \in x_k} d(x,x_k).
\end{equation}\]</span></p>
<p>This minimization ensures that points in a cluster to be close to
eachother, but points from two different clusters to be far apart as
possible. K-means will converge to some local minimum of (3), but this
may not be the global minimum.</p>
<div id="a-clear-example-in-two-d" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> A clear example in
two-d</h2>
<p>Let’s generate some points in which it should be clear how k-means
will work</p>
<pre class="r"><code>pts &lt;- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(pts) &lt;- c(&quot;x&quot;, &quot;y&quot;)

X = data.frame(pts)

#testing for 8 clusters
cl &lt;- kmeans(pts, 2)

X$cluster = as.character(cl$cluster)

X %&gt;% ggplot(aes(x,y, color = cluster)) + geom_point()</code></pre>
<p><img src="newlab18_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<blockquote>
<p><strong>Q:</strong> Obviously, <span class="math inline">\(k =
8\)</span> isn’t the right <span class="math inline">\(k\)</span>. What
is the correct <span class="math inline">\(k\)</span>?</p>
</blockquote>
</div>
<div id="iris-dataset-and-k-means" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Iris dataset and
k-means</h2>
<p>What happens if we try classifying the iris dataset by k-means? This
is a bit odd, since we are not using the labels at all!</p>
<pre class="r"><code>clust = kmeans(scale(iris[-5]), 3)

clust$cluster</code></pre>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2
##  [75] 2 3 3 3 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3
## [112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 3 3 3 3 3 3 2 2 3 3 3 2 3 3 3 2 3 3 3 2 3
## [149] 3 2</code></pre>
<pre class="r"><code>Z = data.frame(clust$cluster, iris$Species)

#Let&#39;s tally what each cluster gets

tally = matrix(0,3,3)

for (i in 1:dim(Z)[1]){
  q = Z[i,1]
  tally[q,1] = tally[q,1]+ (Z[i,2] == &#39;setosa&#39;)
  tally[q,2] = tally[q,2] + (Z[i,2] == &#39;versicolor&#39;)
  tally[q,3] = tally[q,3] +(Z[i,2] == &#39;virginica&#39;)
  
}


print(tally)</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]   50    0    0
## [2,]    0   39   14
## [3,]    0   11   36</code></pre>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
